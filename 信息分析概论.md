## 什么是信息？ 
**凡是在一种情况下能减少不确定性的任何事物都叫信息。 ——  Shannon 《通信的数学理论》**

> 美国学者 E·M·罗杰斯进一步阐述了香农的信息概念：「信息」被定义为一种有别于物质-能源的东西，在需要做出决策时，有一个选择存在于一批选择之中。在这种情况下，信息影响不确定性。因此，信息是为了减少不确定性。
> 
> 例如，天气预报是一种信息。它不能 100% 预测未来，但能减少预测的不确定性。在天气多变的地区，依靠天气预报，人们可以更好地计划今天穿什么衣物。

## 最大信息熵原理 (MIP)
「熵」的概论最早来源于 19 世纪热力学研究。香农的信息概念建立在物理学家 Ludwig Boltzmann 提出熵的等式基础上，后者是热力学第二定律的组成部分。

​        在香农信息论中，「信息量」用「熵」来表达，表示的是某系统信息丧失或不确定性的一个度量。不确定性越大，信息量就越大，熵值越大；反之则越小。使得「熵」最大的事情，最有可能接近其真实状态。

此外，香农还定义了信息测量单位「比特」 —— 比特没有颜色、尺寸或重量，能以光速传播。它好比人体内的 DNA ，是信息的最小单位。

## 信息分布的六大数学原理

#### 布拉德福定律
​        也称文献分散定律。告诉了我们一个重要的知识，即与一个学科相关的文献可以分成三个层级，第一个是跟该学科最相关的核心区，第二是相关区，第三是一般区域。数量关系为：

> 核心区：相关区：非相关区=1：n：n^2^

从核心区、核心期刊入手，获得优质信息的概率会大大提高。

#### 洛特卡定律

​        发表1篇论文的作者占作者总数的60.79%，发表n篇论文的作者数量是发表1篇论文作者数量的1/n^2^

洛特卡定律说明一个学科领域中的核心大牛是有限的。绝大多数作者是不重要的，重要的只有极少数极少数人，找到这些作者，大概率就把握了这个领域的核心信息。

#### 齐夫定律

最早由语言学家乔治·K·齐夫（George Kingsley Zipf）在《最省力原则——人类行为生态学导论》提出。齐夫使用大量数学研究、语言学案例，在大时间周期，使用最省力原则才是取胜策略。

> 齐夫定律可以表述为：在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比。
>
> 也可表述为：如果把一篇较长文章中每个词出现的频次统计起来，按照高频词在前、低频词在后的递减顺序排列，并用自然数给这些词编上等级序号，即频次最高的词等级为1，频次次之的等级为2，……，频次最小的词等级为D。若用f表示频次，r表示等级序号，则有fr=C(C为常数)。
>
> > 数轴表示： *Y轴从下到上是出现次数，X轴从左到右是等级序号。*

齐夫定律给我们什么启发？

- 信息开源长远的效率大于信息封闭。
- 人们更愿意引用自己熟悉的论文。
- 当系统中其他人都越来越倾向于采取省力气的做法，你采取逆流而上的策略，可能更容易构建自己的核心竞争力。

#### 文献增长率

以科技文献量为纵轴，以历史年代为横轴，信息量随时间而不断增加。

> Q(0)=a * exp(1 * 0)，Q(10)=a * exp(1 * 10)

由于文献不可能无限增长，普莱斯定律也有了新的修正。

#### 文献老化率

信息的价值随时间推移而不断下降不同学科领域的文献老化速度是不一样的，于是决定了采取的学习策略不一样。老化速度快的学科就需要优先看新的论文。

#### 信息计量的大一统模型

叶鹰老师及合作者，用波动-扩散方程尝试把五个模型进行统一的整合。

## 构建你的信息分析系统

信息分析四步走：信息获取；信息整理；信息加工；信息报告。

> ##### 信息分析流程
>
> 信息的目的是减少判断不确定性，增强决策准确性。具体来说包括确认事实，做出准确、可靠、有效的推理（包含假设、评估、结论、预测），辅助最终决策。
>
> 我们将信息分析流程拆解为关键四步：收集 - 整理 - 加工 - 输出报告。因运用场景不同，会有不同偏重。如寻找到目标客户联系方式，通常只需完成第一步；向上级汇报某产品市场需求分析，则需要全部四步。

