## 提示

本卡片用于记录第一周学习心得，教过去的自己学信息分析，可包括本次作业刻意练习之处、探索过程、可积累的清单/经验等，或者希望同大家讨论的要点。



## 背景

在做完基础和进阶作业①之后，开始制作笔记卡。考虑到第一周的理论知识和工具非常重要，会影响到后几周的学习效率。所以边制作，边二刷卡包，试图更清晰的理解知识点，构建理论框架。并就作业中的问题，对相关知识点进行深挖，以符合实际使用的需求。



## Ch1 重难点分布

> 学习第一周的信息分析概论，理论部分最关键的是全局认识、交叉验证。
>
> 实践策略，最关键的知识有：
>
> 1、上位词、下位词 
> 2、Zotero 及其 translators
> 3、github 及其 awesome 清单 
>
> 这三个知识点非常反常识，且实用。
>
> ![img](https://static.openmindclub.com/2018-12-07-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-07%20%E4%B8%8B%E5%8D%883.24.47.png)
>
> 其中，「重要」这一栏中，标星表示「非常重要」；「难度」这一栏，一颗星表示「有难度」，两颗星表示「挺难的」。

解读这部分，将 `全局认识` `交叉验证` `检索关键词（上、下位词）` `Zotero` 列为学习重点。（卡包中提到的GitHub应该是前几届的内容，第三届不学习，但是这个工具及其背后的理念非常重要，考虑以后自学。）



## 卡片一：基础理论

> 学习建议：**`练习`**   **`交流`**   **`输出`** ，即使是理论知识，也一定要想办法实践。不用，永远不知道该怎么用，用的时候会有什么问题，这一点在作业中就发现了。

#### 信息分布规律的三大维度：

![img](https://static.openmindclub.com/openmindclub/2018-10-01-PicIA002CH1Yang%20.005.jpeg) 

> 了解了信息分布有时间维度，你未来检索、搜集信息时可以如何应用呢？
>
> 比如：
>
> - 在获取人物信息的时候，注意人物所处的时代、生卒年份
> - 在获取学科信息时，了解一个学科、多个学科发展的时间线、里程碑、转折点
> - 在获取产品、公司等信息时，了解其发展周期
> - ……

分析信息在时间线上的重要节点。（待补充）

> 了解了信息分布有空间维度，你未来检索、搜集信息时可以如何应用呢？
>
> 比如
>
> - 了解一个新领域时，思考优质信息最密集的城市、机构、产品是什么
> - 了解一个新领域时，思考其结构分布是怎样的
> - ……

分析信息的空间布局，既有物理层面，也有非物理层面。（待补充）

> 学习了「变量三方式」，你看待信息、概念的视角有什么变化？
>
> 信息分析的变量关系维度 —— 变量三方式
>
> - 变量方式一：变量的定义是什么？
> - 变量方式二：它作为因变量会被哪些关键变量影响？
> - 变量方式三：它作为自变量会对影响其他什么变量？

找到信息中的关键变量并加以定义，研究各变量间的关系。（待补充）



#### 什么是信息？

> 凡是在一种情况下能减少不确定性的任何事物都叫信息。 ——  Shannon 《通信的数学理论》

信息可以减少不确定性。事件的不确定性是以其发生机率来量测，发生机率越高，不确定性越低，事件的不确定性越高，越需要额外的信息减少其不确定性。



#### 最大信息熵原理 (MIP)

> 「熵」的概论最早来源于 19 世纪热力学研究。香农的信息概念建立在物理学家 Ludwig Boltzmann 提出熵的等式基础上，后者是热力学第二定律的组成部分。
>
> 在香农信息论中，「信息量」用「熵」来表达，表示的是某系统信息丧失或不确定性的一个度量。不确定性越大，信息量就越大，熵值越大；反之则越小。使得「熵」最大的事情，最有可能接近其真实状态。
>
> 此外，香农还定义了信息测量单位「比特」 —— 比特没有颜色、尺寸或重量，能以光速传播。它好比人体内的 DNA ，是信息的最小单位。

熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大。熵越大——不确定性越大——信息就越多——越能描述目标的真实情况。



#### 信息分布六大原理

1. 布拉德福定律

   如果将期刊按其刊载某学科专业论文的数量多少，以递减顺序排列，划分为三个层级，并使各个区的文章数量相等。那么可以将一个学科的相关文献分为：核心区；相关区；一般区域。他们之间的数量关系为1：n：n^2^ ，其中n为Bradford系数。这说明特定学科之最相关文献集中于少数的核心期刊之中。我们从核心区、核心期刊入手，获得优质信息的概率会大大提高。

2. 洛特卡定律

   只发表一篇论文的作者总数，约是全部作者总数的60%。假设发表n篇论文的作者数为X(n)，发表一篇论文的作者与之的关系为X(n)=X(1)/n^2^ 。这个定律说明一个学科领域中的核心大牛是有限的。绝大多数作者不重要的，重要的只有极少数极少数人，找到这些作者，大概率就把握了这个领域的核心信息。

3. 齐夫定律

   如果有一个包含n个词的文章，将这些词按其出现的频次递减地排序，那么序号r和其出现频次f之积fr，将近似地为一个常数。对其成因，有一个重要的“省力法则”假说。认为在语言交流过程中，“省力法则”同时体现在说话人和听话人身上。说话人希望组成语言的词少，而且一词多义,以节省其精力。听话人认为最好是一词一义,使听到的词与其确切涵义容易匹配,减少他理解的功夫。这2种节省精力的倾向最后平衡的结果,便是词频的那种双曲线型分布。

   > 在大时间周期，使用最省力原则才是取胜策略。
   >
   > 齐夫定律给我们什么启发？
   >
   > - 信息开源长远的效率大于信息封闭。
   > - 人们更愿意引用自己熟悉的论文。-
   > - 当系统中其他人都越来越倾向于采取省力气的做法，你采取逆流而上的策略，可能更容易构建自己的核心竞争力。

4. 文献增长率

   > 普赖斯在其经典著作《巴比伦以来的科学》中考察统计了科学期刊的增长情况。以科技文献量为纵轴，以历史年代为横轴，不同年代的科技文献量的变化过程表现为一根光滑的曲线，这条曲线十分近似地表示了科技文献量指数增长的规律。
   >
   > 由于文献不可能无限增长，普莱斯定律也有了新的修正。

   以上是卡包中的资料，但是在做题时，发现需要用到普赖斯定律，检索到如下内容：

   > 在《小科学，大科学》一书中，普赖斯写道：“科学家的总人数，大致是按杰出科学家人数的平方增长的。”所谓普赖斯定律（Price Law），即科学家总人数开平方，所得到的人数撰写了全部科学论文的50%。如果设最高产的那位科学家所发表的论文数为nmax，将科学家们发表论文的总数记为x(1， nmax)，则普赖斯定律可用下式表示：
   >
   > (1/2)x(1， nmax) = x(m， nmax) = x(1，m)
   >
   > 式中，m为普赖斯假定的这样一个数，即个人的论文数大于m的科学家们所发表的论文总数恰好等于全部论文总数的一半，而式中x(m， nmax)的意义恰好表征了这一半论文。  
   >
   > 这里的关键是如何确定m的数值？普赖斯根据洛氏定律，借用数学结论，经推导得出：
   >
   > m≈0 .749(nmax 1/2)。
   >
   > 这是洛特卡定律的一个重要推论。这说明，发表了0.749(nmax 1/2)篇以上论文的科学家们所发表的论文总数等于全部论文总数的一半；或者说，杰出科学家中最低产的那位科学家所发表的论文数，等于最高产科学家发表论文数的平方根的0.749倍。普赖斯还曾试图找出全体科学家总数中杰出科学家的比例关系。经过进一步推导和计算，得出：
   >
   > R≈0.812/nmax 1/2
   >
   > 式中，R是杰出科学家人数与全体科学家总数之比。这是普赖斯得出的洛特卡定律的又一个重要推论。

   至于普赖斯指数，还是没搞懂。（待补充）


5. 文献老化率

   > 信息价值随时间推移而不断下降的规律

   不同学科领域的文献老化速度是不一样的，于是决定了采取的学习策略不一样。老化速度快的学科就需要注重文献更新和迭代的变化，关注与原始文献相比最新的突破与进展。

6. 

